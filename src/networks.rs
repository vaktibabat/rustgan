// This first import is only used in debug mode
use crate::{
    neural_net::{Loss, NeuralNet},
    optim::{Adam, SGD},
};
use ndarray::{Array1, Array2, ArrayView2, Axis};
use rand::{thread_rng, Rng};
use rand_distr::{Distribution, Normal};
use std::{fs::File, io::Write};

const NOISE_DIM: usize = 100;
const IMAGE_SIZE: usize = 28;

// Helper trait on Array2 in order to sample random batches from the dataset
// and to sample noise from the dataset
trait SampleRandom {
    // Sample m random rows from this matrix
    fn random_sample(&self, m: usize) -> Self;
    // Sample m random gaussian noise vectors each of dimension n
    fn noise_sample(m: usize, n: usize) -> Self;
}

impl SampleRandom for Array2<f64> {
    fn random_sample(&self, m: usize) -> Self {
        let mut rng = thread_rng();
        let mut idxs = vec![];
        let mut sample = Array2::zeros((0, self.ncols()));

        // Generate the random indexes
        for _ in 0..m {
            idxs.push(rng.gen_range(0..self.nrows()));
        }
        // Push the corresponding rows to a new matrix
        for idx in idxs {
            let row = self.row(idx);
            sample.push_row(row).unwrap();
        }

        sample
    }

    fn noise_sample(m: usize, n: usize) -> Self {
        let dist = Normal::new(0f64, 1f64).unwrap();
        let mut rng = thread_rng();
        let noise = Array2::from_shape_fn((m, n), |_| dist.sample(&mut rng));

        noise
    }
}

//-----------------START NETWORK TYPES---------------------
pub struct FullyConnected {
    net: NeuralNet,
}

pub struct GenerativeAdverserial {
    pub discriminator: NeuralNet,
    pub generator: NeuralNet,
    // The user can specify a path to save the intermediate images
    // generated by the model if they want to
    img_path: Option<String>,
    // Whether to print intermediate losses
    print_losses: bool,
}
//-----------------END NETWORK TYPES-----------------------
//-----------------START FULLY CONNECTED-------------------
impl FullyConnected {
    pub fn new(
        num_epochs: usize,
        batch_size: usize,
        learning_rate: f64,
        loss: Loss,
    ) -> FullyConnected {
        let net = NeuralNet::new(num_epochs, batch_size, learning_rate, loss);

        FullyConnected { net }
    }

    // Compute the gradient of this network's loss (MSE/Cross Entropy) WRT the output
    fn upstream_loss(
        loss: Loss,
        net_out: &Array2<f64>,
        ground_truth: &ArrayView2<f64>,
    ) -> Result<Array2<f64>, ()> {
        match loss {
            Loss::Mse => {
                // Number of elements in the GT
                let m = ground_truth.ncols() as f64;

                Ok((2f64 / m) * (net_out - ground_truth))
            }
            Loss::CrossEntropy => Ok(net_out - ground_truth),
            _ => {
                // GAN loss types are unimplemented for a FCN
                Err(())
            }
        }
    }
}

impl FullyConnected {
    pub fn fit(&mut self, x: Array2<f64>, y: Array2<f64>) {
        // Extract training hyperparameters
        let num_epochs = self.net.num_epochs();
        let learning_rate = self.net.learning_rate();
        let batch_size = self.net.batch_size();
        let loss = self.net.loss();
        let mut net = &mut self.net;
        let mut optimizer = Adam::new(learning_rate, 0.9, 0.9, net);
        let mut sgd = SGD::new(0.001);

        // Training loop
        for num_epoch in 0..num_epochs {
            for (input_batch, gt_batch) in x
                .axis_chunks_iter(Axis(0), batch_size)
                .zip(y.axis_chunks_iter(Axis(0), batch_size))
            {
                // Compute the forward pass on the input batch
                net.forward(&input_batch.view(), true);
                // Compute the gradients using the backward pass
                let net_out = &net.cache_last();
                let dy = FullyConnected::upstream_loss(loss, net_out, &gt_batch).unwrap();
                let mut gradients = net.backward(dy).0;
                gradients.reverse();
                // Perform GD step
                optimizer.optimize(net, gradients);
                //sgd.optimize(net, gradients);
            }

            println!("Finished epoch {}", num_epoch);

            // Print our current loss
        }
    }

    fn predict(&mut self, x: Array2<f64>) -> Array2<f64> {
        let net = &mut self.net;

        // For FCN, the prediction is just the output of the net
        net.forward(&x.view(), false)
    }
}
//-------------------END FULLY CONNECTED--------------
//-------------------START GAN------------------------
impl GenerativeAdverserial {
    // The upstream gradient of the Discriminator's loss WRT the discriminator's output on
    // a batch containing generated samples and real samples
    fn upstream_disc(net_out: &Array2<f64>) -> Array2<f64> {
        // The first half of the outputs are probabilities for real training samples
        // The other half are probabilities for samples generated from noise by the generator
        let m = net_out.nrows() / 2;
        let mut out_grad = Array2::zeros((0, 1));

        // The upper half
        for i in 0..m {
            let prob_xi = net_out.row(i)[0];
            // Derivative of log(D(x_i)) WRT D(x_i) is 1 / D(x_i)
            let dprob_xi = 1f64 / prob_xi;
            let to_push = Array1::from_elem(1, dprob_xi);
            out_grad.push_row(to_push.view()).unwrap();
        }
        // The lower half
        for i in m..2 * m {
            let prob_gzi = net_out.row(i)[0];
            // Derivative of log(1 - D(G(z_i))) WRT D(G(z_i)) is 1 / (D(G(z_i)) - 1)
            let dprob_gzi = 1f64 / (prob_gzi - 1f64);
            let to_push = Array1::from_elem(1, dprob_gzi);
            out_grad.push_row(to_push.view()).unwrap();
        }

        -1f64 * (1f64 / m as f64) * out_grad
    }

    // The gradient of the Generator's loss WRT each pixel in each image
    // in the generator's batch
    fn upstream_gen(
        net_out: &Array2<f64>,
        disc_out: &Array2<f64>,
        dy_dgzi: &Array2<f64>,
    ) -> Array2<f64> {
        // net_out contains the generated images, of shape (batch_size, img_size)
        let m = net_out.nrows();
        // The gradient of the loss WRT each pixel in each output image
        // shape: (batch_size, img_size)
        let mut out_grad = Array2::zeros((0, IMAGE_SIZE * IMAGE_SIZE));

        // Compute the gradients
        for i in 0..m {
            let prob_gzi = disc_out.row(i)[0];
            // Derivative of log(D(G(z_i))) WRT D((G(z_i))) is 1 / D(G(z_i)))
            let dprob_gzi = 1f64 / prob_gzi;
            // The gradient of the loss WRT counterfeit i
            let dy_curr = dy_dgzi.row(i);
            let to_push = dy_curr.map(|x| x * dprob_gzi);
            out_grad.push_row(to_push.view()).unwrap();
        }

        (1f64 / m as f64) * out_grad
    }

    // The loss of the discriminator on some input batch composed of real and generated samples
    // the first batch_size samples are real, while the rest are counterfeit
    fn disc_loss(net_out: &Array2<f64>) -> f64 {
        let m = net_out.nrows() / 2;
        let real_range: Vec<usize> = (0..m).collect();
        let fake_range: Vec<usize> = (m..2 * m).collect();
        let real_samples = net_out.select(Axis(0), &real_range);
        let fake_samples = net_out.select(Axis(0), &fake_range);

        // We take the sum of the logs of the predicted probabilties of each real sample
        // plus the sum of the logs of the complement of the predicted probabilities for each fake sample
        let loss =
            real_samples.map(|x| x.log2()).sum() + fake_samples.map(|x| (1f64 - x).log2()).sum();

        (1f64 / m as f64) * loss
    }

    // The loss of the generator
    fn gen_loss(net_out: &Array2<f64>) -> f64 {
        let m = net_out.nrows();
        let fake_range: Vec<usize> = (0..m).collect();
        let fake_samples = net_out.select(Axis(0), &fake_range);
        let loss = fake_samples.map(|x| x.log2()).sum();

        (1f64 / m as f64) * loss
    }
}

impl GenerativeAdverserial {
    // When we fit a GAN, we only need a list of images
    // so there aren't any GT
    pub fn fit(&mut self, x: Array2<f64>) {
        // Extract training hyperparameters
        let num_epochs = self.discriminator.num_epochs();
        let learning_rate = self.discriminator.learning_rate();
        let batch_size = self.discriminator.batch_size();
        let disc = &mut self.discriminator;
        let gen = &mut self.generator;
        let batches_per_epoch = x.nrows() / batch_size;
        let mut losses_file = match self.print_losses {
            true => Some(File::create("./losses.txt").unwrap()),
            false => None,
        };
        // Adam optimizers for the discriminator and the generator
        //let mut disc_optim = Adam::new(learning_rate, 0.5, 0.5, &disc);
        //let mut gen_optim = Adam::new(learning_rate, 0.5, 0.5, &gen);
        let disc_optim = SGD::new(learning_rate);
        let gen_optim = SGD::new(learning_rate);

        for num_epoch in 0..num_epochs {
            for _ in 0..batches_per_epoch {
                // Begin discriminator training step
                // Sample minibatch of m training samples
                let mut input_batch = x.random_sample(batch_size);
                // Sample minibatch of m noise vectors, each of NOISE_DIM dimensions
                let noise_batch = Array2::noise_sample(batch_size, NOISE_DIM);
                let gen_samples = gen.forward(&noise_batch.view(), false);
                // Concatente them into one matrix, since we want the discriminator
                // to run both on the input and on the noise and produce probabilities for each sample
                input_batch.append(Axis(0), gen_samples.view()).unwrap();
                // Run the discriminator
                disc.forward(&input_batch.view(), true);
                // Get the gradients by running the backward pass
                let net_out = &disc.cache_last();
                let dy = GenerativeAdverserial::upstream_disc(net_out);
                let mut gradients = disc.backward(dy).0;
                gradients.reverse();
                // Perform a GD step on the discriminator
                disc_optim.optimize(disc, gradients);
                // End discriminator training step
                // Begin generator training step
                // Sample minibatch of m noise vectors, each of NOISE_DIM dimensions
                let noise_batch = Array2::noise_sample(batch_size, NOISE_DIM);
                // Run the generator on the noise to generate new samples mimicing the training data
                let gen_samples = gen.forward(&noise_batch.view(), true);
                // Run the discriminator on the generated samples
                disc.forward(&gen_samples.view(), true);
                // The output of the discriminator on the generated images
                let disc_out = disc.cache_last().clone();
                // The first element in the discriminator's cache (the gradient of the discriminator)
                // WRT the generated samples
                // is used to compute the gradient of the generator
                let dy = GenerativeAdverserial::upstream_disc(&disc_out);
                let gen_grads = disc.backward(dy).1;
                // Get the gradient by running the backward pass
                let dy = GenerativeAdverserial::upstream_gen(&gen_samples, &disc_out, &gen_grads);
                let mut gradients = gen.backward(dy).0;
                gradients.reverse();
                // Perform a GD step on the generator
                gen_optim.optimize(gen, gradients);
            }

            // Test discriminator on a random batch
            let noise_batch = Array2::noise_sample(batch_size, NOISE_DIM);
            let gen_samples = gen.forward(&noise_batch.view(), false);
            let mut input_batch = x.random_sample(batch_size);
            input_batch.append(Axis(0), gen_samples.view()).unwrap();
            let disc_out = disc.forward(&input_batch.view(), false);
            // Test the discriminator on the fake batch and use that to compute the generator loss
            let disc_out_fake = disc.forward(&gen_samples.view(), false);
            //let disc_res = disc.forward(&gen_samples.view(), false);
            if self.print_losses {
                let disc_loss = GenerativeAdverserial::disc_loss(&disc_out);
                let gen_loss = GenerativeAdverserial::gen_loss(&disc_out_fake);
                println!("Completed epoch {}", num_epoch);
                println!("Discriminator loss: {},", disc_loss);
                println!("Generator loss: {}", gen_loss);
                // Write them to a file
                losses_file
                    .as_mut()
                    .unwrap()
                    .write_all(format!("{}\t{}\n", disc_loss, gen_loss).as_bytes())
                    .unwrap();
            }

            if let Some(inter_img_path) = &self.img_path {
                // Write to disk
                let mut curr_file =
                    File::create(format!("{}/{}", inter_img_path, num_epoch)).unwrap();

                for x in gen_samples.row(0) {
                    curr_file.write_all(format!("{},", x).as_bytes()).unwrap();
                }
            }
        }
    }

    pub fn generate(&mut self, x: Array2<f64>) -> Array2<f64> {
        self.generator.forward(&x.view(), false)
    }
}

impl GenerativeAdverserial {
    pub fn new(
        num_epochs: usize,
        batch_size: usize,
        learning_rate: f64,
        img_path: Option<String>,
    ) -> GenerativeAdverserial {
        let discriminator = NeuralNet::new(
            num_epochs,
            batch_size,
            learning_rate,
            Loss::GanDiscriminator,
        );
        let generator = NeuralNet::new(num_epochs, batch_size, learning_rate, Loss::GanGenerator);

        GenerativeAdverserial {
            discriminator,
            generator,
            img_path,
            print_losses: true,
        }
    }
}
//---------------------END GAN---------------------------

#[cfg(test)]
mod tests {
    use ndarray::Array2;
    use rand::{
        distributions::{Distribution, Uniform},
        thread_rng,
    };

    use crate::layers::{Linear, ReLU};

    use super::FullyConnected;

    // Helper trait for generating random matrices
    trait RandomInit {
        fn random(n: usize, m: usize) -> Self;
    }

    impl RandomInit for Array2<f64> {
        fn random(n: usize, m: usize) -> Self {
            let mut rng = thread_rng();
            let dist = Uniform::new(-1f64, 1f64);
            // Create the weight matrix
            Array2::from_shape_fn((n, m), |_| dist.sample(&mut rng))
        }
    }

    #[test]
    fn test_fully_connected() {
        // Input data
        let x = Array2::random(20000, 1);
        // Linear transformation on x - the GT
        let y = x.map(|x| (x / 2f64) + x.exp()) + Array2::from_elem((20000, 1), 3.27);
        // Create our net - only one neuron
        let mut net = FullyConnected::new(3, 32, 0.001, crate::neural_net::Loss::Mse);
        net.net.add_layer(Box::new(Linear::new(1, 500)));
        net.net.add_layer(Box::new(ReLU::new()));
        net.net.add_layer(Box::new(Linear::new(500, 1)));
        // Train it
        net.fit(x, y.clone());
        // Print our weight values - should be w close to 1.337 and b close to 3.27
        println!("Training finished");
        let input_vec =
            Array2::from_shape_vec((200, 1), (-100..100).map(|x| (x as f64) / 100f64).collect())
                .unwrap();
        let output_vec = net.predict(input_vec.clone());
        let correct_vec = input_vec.map(|x| (x / 2f64) + x.exp() + 3.27);

        println!("Output on 0: {}", net.predict(Array2::from_diag_elem(1, 0f64)));
        println!("Output on 1: {}", net.predict(Array2::from_diag_elem(1, 1f64)));

        //println!("Output: {:?}", output_vec);
        //println!("Correct: {:?}", correct_vec);

        // TODO - change this to a more correct test
        // the function should be approximated, measuring the distance between each two outputs is not good
        // because outliers break it
        for (y, z) in correct_vec.iter().zip(output_vec.iter()) {
            assert!((y - z).abs() < 0.1);
        }
    }

    #[test]
    fn my_test() {
        println!("{:?}", 0.001 * (Array2::from_diag_elem(3, 3f64) * Array2::from_diag_elem(3, 3f64)));
        let first_w = Array2::from_diag_elem(3, 3f64);
        let second_w = Array2::from_diag_elem(3, 4f64);
        let dw = first_w / (second_w.sqrt() + 1e-8);

        println!("{:?}", dw);
    }
}
